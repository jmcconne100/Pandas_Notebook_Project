{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNakbUX1yKk9Jfsi4BbOpHS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmcconne100/Pandas_Notebook_Project/blob/main/Schema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dq_contracts.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ----------------------------- Helpers -----------------------------\n",
        "\n",
        "_PANDAS2SCHEMA = {\n",
        "    \"int64\": \"integer\", \"int32\": \"integer\", \"Int64\": \"integer\",\n",
        "    \"float64\": \"float\", \"float32\": \"float\",\n",
        "    \"bool\": \"boolean\",\n",
        "    \"object\": \"string\",\n",
        "    \"category\": \"category\",\n",
        "    \"datetime64[ns]\": \"datetime\",\n",
        "}\n",
        "\n",
        "_ALLOWED_COERCIONS = {\n",
        "    (\"integer\", \"float\"),\n",
        "    (\"integer\", \"string\"),  # safe-ish as text\n",
        "    (\"float\", \"string\"),\n",
        "    (\"boolean\", \"string\"),\n",
        "    (\"datetime\", \"string\"),\n",
        "    (\"category\", \"string\"),\n",
        "    (\"string\", \"category\"),\n",
        "}\n",
        "\n",
        "def _schema_type_from_dtype(dtype: pd.api.types.CategoricalDtype | np.dtype) -> str:\n",
        "    s = str(dtype)\n",
        "    for k, v in _PANDAS2SCHEMA.items():\n",
        "        if k in s:\n",
        "            return v\n",
        "    return \"string\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ColumnConstraint:\n",
        "    name: str\n",
        "    type: str\n",
        "    nullable: bool\n",
        "    min: Optional[float] = None\n",
        "    max: Optional[float] = None\n",
        "    allowed: Optional[List[Any]] = None   # for enums/categories\n",
        "    unique: Optional[bool] = None         # enforces column uniqueness (optional)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TableSchema:\n",
        "    columns: List[ColumnConstraint]\n",
        "    primary_key: Optional[List[str]] = None\n",
        "    description: Optional[str] = None\n",
        "    version: Optional[str] = None\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"columns\": [asdict(c) for c in self.columns],\n",
        "            \"primary_key\": self.primary_key,\n",
        "            \"description\": self.description,\n",
        "            \"version\": self.version,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(d: Dict[str, Any]) -> \"TableSchema\":\n",
        "        cols = [ColumnConstraint(**c) for c in d[\"columns\"]]\n",
        "        return TableSchema(columns=cols, primary_key=d.get(\"primary_key\"),\n",
        "                           description=d.get(\"description\"), version=d.get(\"version\"))\n",
        "\n",
        "\n",
        "# ----------------------------- Core APIs -----------------------------\n",
        "\n",
        "def infer_schema(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    primary_key: Optional[List[str]] = None,\n",
        "    description: Optional[str] = None,\n",
        "    version: Optional[str] = None,\n",
        "    sample_for_ranges: int = 200_000,\n",
        ") -> TableSchema:\n",
        "    \"\"\"\n",
        "    Infer a lightweight, JSON-serializable table schema from a DataFrame.\n",
        "    - Maps pandas dtypes to simple types\n",
        "    - Computes nullability, min/max for numeric/datetime, and allowed values for small-cardinality categoricals\n",
        "    \"\"\"\n",
        "    cols: List[ColumnConstraint] = []\n",
        "    n = len(df)\n",
        "\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        stype = _schema_type_from_dtype(s.dtype)\n",
        "        nullable = bool(s.isna().any())\n",
        "\n",
        "        c = ColumnConstraint(name=col, type=stype, nullable=nullable)\n",
        "\n",
        "        # Ranges\n",
        "        if stype in (\"integer\", \"float\"):\n",
        "            sample = s.dropna()\n",
        "            if len(sample) > sample_for_ranges:\n",
        "                sample = sample.sample(sample_for_ranges, random_state=0)\n",
        "            if not sample.empty:\n",
        "                c.min = float(np.nanmin(sample))\n",
        "                c.max = float(np.nanmax(sample))\n",
        "        elif stype == \"datetime\":\n",
        "            sample = s.dropna()\n",
        "            if not sample.empty:\n",
        "                c.min = pd.to_datetime(sample.min()).value / 1e9  # seconds since epoch\n",
        "                c.max = pd.to_datetime(sample.max()).value / 1e9\n",
        "\n",
        "        # Allowed values for small-cardinality categoricals/strings\n",
        "        unique_ct = s.nunique(dropna=True)\n",
        "        if stype in (\"category\", \"string\", \"boolean\") and unique_ct <= 50:\n",
        "            allowed = s.dropna().unique().tolist()\n",
        "            c.allowed = allowed\n",
        "\n",
        "        # Uniqueness hint if looks like a key\n",
        "        if primary_key and col in primary_key:\n",
        "            c.unique = True\n",
        "        elif unique_ct == n and n > 0:\n",
        "            c.unique = True\n",
        "\n",
        "        cols.append(c)\n",
        "\n",
        "    return TableSchema(columns=cols, primary_key=primary_key, description=description, version=version)\n",
        "\n",
        "\n",
        "def save_schema(schema: TableSchema, path: Union[str, Path]) -> None:\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with p.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(schema.to_dict(), f, indent=2)\n",
        "\n",
        "\n",
        "def load_schema(path: Union[str, Path]) -> TableSchema:\n",
        "    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "        d = json.load(f)\n",
        "    return TableSchema.from_dict(d)\n",
        "\n",
        "\n",
        "def validate_schema(\n",
        "    df: pd.DataFrame,\n",
        "    expected: TableSchema,\n",
        "    *,\n",
        "    strict_types: bool = False,\n",
        "    check_ranges: bool = True,\n",
        "    check_allowed: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validate df against a TableSchema.\n",
        "    - Column presence/absence\n",
        "    - Type compatibility (strict or with allowed coercions)\n",
        "    - Nullability violations\n",
        "    - Range checks for numeric/datetime (if available)\n",
        "    - Allowed value checks for categorical/string columns (if provided)\n",
        "    - Primary key uniqueness if present\n",
        "    Returns a structured report dict with 'ok': bool and detailed findings.\n",
        "    \"\"\"\n",
        "    report = {\n",
        "        \"ok\": True,\n",
        "        \"missing_columns\": [],\n",
        "        \"extra_columns\": [],\n",
        "        \"type_mismatches\": [],\n",
        "        \"nullability_violations\": [],\n",
        "        \"range_violations\": [],\n",
        "        \"allowed_value_violations\": [],\n",
        "        \"primary_key_violations\": None,\n",
        "    }\n",
        "\n",
        "    expected_cols = {c.name: c for c in expected.columns}\n",
        "    # Missing / extra\n",
        "    for col in expected_cols:\n",
        "        if col not in df.columns:\n",
        "            report[\"ok\"] = False\n",
        "            report[\"missing_columns\"].append(col)\n",
        "    for col in df.columns:\n",
        "        if col not in expected_cols:\n",
        "            report[\"extra_columns\"].append(col)\n",
        "\n",
        "    # Per-column checks\n",
        "    for col, constraint in expected_cols.items():\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        s = df[col]\n",
        "        actual_type = _schema_type_from_dtype(s.dtype)\n",
        "\n",
        "        # Type check\n",
        "        if strict_types:\n",
        "            if actual_type != constraint.type:\n",
        "                report[\"ok\"] = False\n",
        "                report[\"type_mismatches\"].append({\"column\": col, \"expected\": constraint.type, \"actual\": actual_type})\n",
        "        else:\n",
        "            if actual_type != constraint.type and (constraint.type, actual_type) not in _ALLOWED_COERCIONS:\n",
        "                report[\"ok\"] = False\n",
        "                report[\"type_mismatches\"].append({\"column\": col, \"expected\": constraint.type, \"actual\": actual_type})\n",
        "\n",
        "        # Nullability\n",
        "        if constraint.nullable is False and s.isna().any():\n",
        "            report[\"ok\"] = False\n",
        "            report[\"nullability_violations\"].append({\"column\": col, \"null_count\": int(s.isna().sum())})\n",
        "\n",
        "        # Ranges\n",
        "        if check_ranges and constraint.type in (\"integer\", \"float\") and (constraint.min is not None or constraint.max is not None):\n",
        "            s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            too_low = s_num < (constraint.min if constraint.min is not None else -np.inf)\n",
        "            too_high = s_num > (constraint.max if constraint.max is not None else np.inf)\n",
        "            n_low, n_high = int(too_low.sum()), int(too_high.sum())\n",
        "            if n_low or n_high:\n",
        "                report[\"ok\"] = False\n",
        "                report[\"range_violations\"].append({\"column\": col, \"below_min\": n_low, \"above_max\": n_high})\n",
        "\n",
        "        # Allowed values\n",
        "        if check_allowed and constraint.allowed is not None:\n",
        "            invalid_mask = ~s.isna() & ~s.astype(object).isin(set(constraint.allowed))\n",
        "            n_bad = int(invalid_mask.sum())\n",
        "            if n_bad:\n",
        "                report[\"ok\"] = False\n",
        "                sample = s[invalid_mask].astype(str).head(10).tolist()\n",
        "                report[\"allowed_value_violations\"].append({\"column\": col, \"bad_count\": n_bad, \"sample_values\": sample})\n",
        "\n",
        "    # Primary key uniqueness\n",
        "    if expected.primary_key:\n",
        "        dup = df.duplicated(subset=expected.primary_key, keep=False).sum()\n",
        "        nulls = df[expected.primary_key].isna().any(axis=1).sum()\n",
        "        if dup or nulls:\n",
        "            report[\"ok\"] = False\n",
        "            report[\"primary_key_violations\"] = {\"duplicate_rows\": int(dup), \"rows_with_null_key\": int(nulls)}\n",
        "        else:\n",
        "            report[\"primary_key_violations\"] = {\"duplicate_rows\": 0, \"rows_with_null_key\": 0}\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "def evolve_schema(old: TableSchema, new: TableSchema) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare two schemas and describe changes (add/remove/modify).\n",
        "    \"\"\"\n",
        "    old_cols = {c.name: c for c in old.columns}\n",
        "    new_cols = {c.name: c for c in new.columns}\n",
        "\n",
        "    added = [c for c in new_cols if c not in old_cols]\n",
        "    removed = [c for c in old_cols if c not in new_cols]\n",
        "    modified: List[Dict[str, Any]] = []\n",
        "\n",
        "    for name in set(old_cols).intersection(new_cols):\n",
        "        a, b = old_cols[name], new_cols[name]\n",
        "        diffs = {}\n",
        "        for field in [\"type\", \"nullable\", \"min\", \"max\", \"allowed\", \"unique\"]:\n",
        "            if getattr(a, field) != getattr(b, field):\n",
        "                diffs[field] = {\"old\": getattr(a, field), \"new\": getattr(b, field)}\n",
        "        if diffs:\n",
        "            modified.append({\"column\": name, \"changes\": diffs})\n",
        "\n",
        "    return {\"added\": added, \"removed\": removed, \"modified\": modified}\n",
        "\n",
        "\n",
        "def enforce_contract(\n",
        "    df: pd.DataFrame,\n",
        "    contract: TableSchema,\n",
        "    *,\n",
        "    cast_strings_to: Optional[Dict[str, str]] = None,   # e.g., {\"price\": \"float\"}\n",
        "    fill_defaults: Optional[Dict[str, Any]] = None,     # e.g., {\"country\": \"US\"}\n",
        "    clip_numeric_to_range: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply a contract to a DataFrame:\n",
        "    - Add missing columns with defaults (None if not supplied)\n",
        "    - Cast columns to target types when safe\n",
        "    - Clip numeric values to [min, max] if specified (optional)\n",
        "    - Keep only columns present in the contract (in order)\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    fill_defaults = fill_defaults or {}\n",
        "    cast_strings_to = cast_strings_to or {}\n",
        "\n",
        "    # Ensure columns exist\n",
        "    for c in contract.columns:\n",
        "        if c.name not in out.columns:\n",
        "            out[c.name] = fill_defaults.get(c.name, np.nan if c.nullable else None)\n",
        "\n",
        "    # Cast basic types\n",
        "    for c in contract.columns:\n",
        "        if c.name not in out.columns:\n",
        "            continue\n",
        "        t = c.type\n",
        "        s = out[c.name]\n",
        "        try:\n",
        "            if t == \"integer\":\n",
        "                out[c.name] = pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
        "            elif t == \"float\":\n",
        "                out[c.name] = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
        "            elif t == \"boolean\":\n",
        "                out[c.name] = s.astype(\"boolean\")\n",
        "            elif t == \"datetime\":\n",
        "                out[c.name] = pd.to_datetime(s, errors=\"coerce\")\n",
        "            elif t == \"category\":\n",
        "                out[c.name] = s.astype(\"category\")\n",
        "            elif t == \"string\":\n",
        "                # Optionally cast specific strings to numeric\n",
        "                if c.name in cast_strings_to:\n",
        "                    tgt = cast_strings_to[c.name]\n",
        "                    if tgt in (\"integer\", \"float\"):\n",
        "                        out[c.name] = pd.to_numeric(s, errors=\"coerce\")\n",
        "                        if tgt == \"integer\":\n",
        "                            out[c.name] = out[c.name].astype(\"Int64\")\n",
        "                    else:\n",
        "                        out[c.name] = s.astype(\"string\")\n",
        "                else:\n",
        "                    out[c.name] = s.astype(\"string\")\n",
        "            else:\n",
        "                out[c.name] = s  # leave as-is\n",
        "        except Exception:\n",
        "            # leave column unchanged if cast fails globally\n",
        "            pass\n",
        "\n",
        "        # Clip numerics if desired\n",
        "        if clip_numeric_to_range and t in (\"integer\", \"float\") and (c.min is not None or c.max is not None):\n",
        "            out[c.name] = pd.to_numeric(out[c.name], errors=\"coerce\")\n",
        "            if c.min is not None:\n",
        "                out[c.name] = out[c.name].clip(lower=c.min)\n",
        "            if c.max is not None:\n",
        "                out[c.name] = out[c.name].clip(upper=c.max)\n",
        "\n",
        "        # Enforce allowed set (if provided)\n",
        "        if c.allowed is not None:\n",
        "            mask = ~out[c.name].isna() & ~out[c.name].astype(object).isin(set(c.allowed))\n",
        "            if mask.any():\n",
        "                # Replace invalid with NaN to avoid bad values downstream\n",
        "                out.loc[mask, c.name] = np.nan\n",
        "\n",
        "    # Keep only contract columns (and in that order)\n",
        "    col_order = [c.name for c in contract.columns]\n",
        "    out = out[col_order]\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "wWhmx8fg4SA8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "te9fmLZ23lpU"
      },
      "outputs": [],
      "source": [
        "# dq_checks.py\n",
        "from __future__ import annotations\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from scipy import stats\n",
        "    _HAS_SCIPY = True\n",
        "except Exception:\n",
        "    _HAS_SCIPY = False\n",
        "\n",
        "\n",
        "def check_uniqueness(df: pd.DataFrame, keys: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Check if `keys` uniquely identify rows.\n",
        "    Returns counts of duplicate key rows and null-key rows.\n",
        "    \"\"\"\n",
        "    dup_rows = int(df.duplicated(subset=keys, keep=False).sum())\n",
        "    null_key_rows = int(df[keys].isna().any(axis=1).sum())\n",
        "    return {\"duplicate_rows\": dup_rows, \"rows_with_null_key\": null_key_rows}\n",
        "\n",
        "\n",
        "def check_referential_integrity(\n",
        "    child: pd.DataFrame,\n",
        "    parent: pd.DataFrame,\n",
        "    fk_cols: List[str],\n",
        "    pk_cols: Optional[List[str]] = None,\n",
        ") -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Check that child[fk_cols] exists in parent[pk_cols].\n",
        "    If pk_cols not provided, uses fk_cols in parent as PK.\n",
        "    Returns number of child rows violating FK constraint.\n",
        "    \"\"\"\n",
        "    pk_cols = pk_cols or fk_cols\n",
        "    child_keys = child[fk_cols].drop_duplicates()\n",
        "    parent_keys = parent[pk_cols].drop_duplicates()\n",
        "\n",
        "    merged = child_keys.merge(parent_keys, left_on=fk_cols, right_on=pk_cols, how=\"left\", indicator=True)\n",
        "    missing = int((merged[\"_merge\"] == \"left_only\").sum())\n",
        "    return {\"missing_fk_rows\": missing, \"child_distinct_keys\": int(len(child_keys)), \"parent_distinct_keys\": int(len(parent_keys))}\n",
        "\n",
        "\n",
        "def detect_distribution_drift(\n",
        "    df_new: pd.DataFrame,\n",
        "    df_ref: pd.DataFrame,\n",
        "    columns: Optional[List[str]] = None,\n",
        "    *,\n",
        "    alpha: float = 0.05,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare distributions between df_new and df_ref.\n",
        "    - Numeric columns: two-sample KS test (if SciPy available), else AD-hoc Wasserstein/quantile distance fallbacks.\n",
        "    - Categorical columns: chi-square test on frequency tables.\n",
        "    Returns a DataFrame with per-column statistic, p-value, and drift flag (p < alpha).\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = sorted(set(df_new.columns).intersection(df_ref.columns))\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for col in columns:\n",
        "        a = df_ref[col].dropna()\n",
        "        b = df_new[col].dropna()\n",
        "\n",
        "        if a.empty or b.empty:\n",
        "            records.append({\"column\": col, \"type\": \"unknown\", \"test\": None, \"statistic\": np.nan, \"p_value\": np.nan, \"drift\": False})\n",
        "            continue\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b):\n",
        "            if _HAS_SCIPY:\n",
        "                stat, p = stats.ks_2samp(a, b)\n",
        "                records.append({\"column\": col, \"type\": \"numeric\", \"test\": \"KS\", \"statistic\": float(stat), \"p_value\": float(p), \"drift\": bool(p < alpha)})\n",
        "            else:\n",
        "                # Fallback: simple Earth Mover's (Wasserstein) distance normalized by IQR\n",
        "                from numpy import quantile\n",
        "                emd = np.abs(np.mean(a) - np.mean(b))\n",
        "                iqr = (quantile(a, 0.75) - quantile(a, 0.25)) + 1e-9\n",
        "                stat = float(emd / iqr)\n",
        "                # Heuristic p: not a true p-value; flag drift if stat > 0.5\n",
        "                records.append({\"column\": col, \"type\": \"numeric\", \"test\": \"heuristic_EMD\", \"statistic\": stat, \"p_value\": np.nan, \"drift\": bool(stat > 0.5)})\n",
        "        else:\n",
        "            # Categorical: chi-square on value counts aligned\n",
        "            vc_a = a.astype(str).value_counts()\n",
        "            vc_b = b.astype(str).value_counts()\n",
        "            idx = sorted(set(vc_a.index).union(vc_b.index))\n",
        "            obs = np.vstack([vc_a.reindex(idx, fill_value=0).values, vc_b.reindex(idx, fill_value=0).values])\n",
        "            if _HAS_SCIPY:\n",
        "                chi2, p, dof, _ = stats.chi2_contingency(obs)\n",
        "                records.append({\"column\": col, \"type\": \"categorical\", \"test\": \"chi2\", \"statistic\": float(chi2), \"p_value\": float(p), \"drift\": bool(p < alpha)})\n",
        "            else:\n",
        "                # Heuristic: Jensen-Shannon distance proxy via normalized counts\n",
        "                pa = obs[0] / max(obs[0].sum(), 1)\n",
        "                pb = obs[1] / max(obs[1].sum(), 1)\n",
        "                stat = float(np.sqrt(((pa - pb) ** 2).sum()))\n",
        "                records.append({\"column\": col, \"type\": \"categorical\", \"test\": \"heuristic_L2\", \"statistic\": stat, \"p_value\": np.nan, \"drift\": bool(stat > 0.25)})\n",
        "\n",
        "    return pd.DataFrame.from_records(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build reference data\n",
        "np.random.seed(0)\n",
        "df_ref = pd.DataFrame({\n",
        "    \"id\": range(1, 101),\n",
        "    \"age\": np.random.normal(35, 8, 100).round(1),\n",
        "    \"country\": np.random.choice([\"US\", \"UK\", \"CA\"], 100, p=[0.6, 0.25, 0.15]),\n",
        "    \"joined\": pd.date_range(\"2024-01-01\", periods=100, freq=\"D\"),\n",
        "})\n",
        "schema_ref = infer_schema(df_ref, primary_key=[\"id\"], description=\"Users ref v1\", version=\"1.0\")\n",
        "\n",
        "print(\"Inferred schema (truncated):\")\n",
        "print(schema_ref.to_dict()[\"columns\"][:3])\n",
        "\n",
        "# Save / load\n",
        "save_schema(schema_ref, \"schema_users_v1.json\")\n",
        "schema_loaded = load_schema(\"schema_users_v1.json\")\n",
        "\n",
        "# New data with a couple issues: extra column, age spike, a bad country value, duplicate id\n",
        "df_new = df_ref.copy()\n",
        "df_new.loc[0, \"id\"] = 1   # duplicate id on purpose\n",
        "df_new.loc[5:10, \"age\"] = 120\n",
        "df_new.loc[20, \"country\"] = \"DE\"  # invalid category\n",
        "df_new[\"extra_col\"] = 1\n",
        "\n",
        "report = validate_schema(df_new, schema_loaded, strict_types=False, check_ranges=True, check_allowed=True)\n",
        "print(\"\\nValidation report ok?:\", report[\"ok\"])\n",
        "print(\"Missing columns:\", report[\"missing_columns\"])\n",
        "print(\"Extra columns:\", report[\"extra_columns\"])\n",
        "print(\"Type mismatches:\", report[\"type_mismatches\"])\n",
        "print(\"Nullability violations:\", report[\"nullability_violations\"])\n",
        "print(\"Range violations:\", report[\"range_violations\"])\n",
        "print(\"Allowed value violations:\", report[\"allowed_value_violations\"])\n",
        "print(\"PK violations:\", report[\"primary_key_violations\"])\n",
        "\n",
        "# Enforce contract (coerce, clip, drop extra)\n",
        "df_enforced = enforce_contract(df_new, schema_loaded)\n",
        "print(\"\\nEnforced DataFrame columns:\", list(df_enforced.columns))\n",
        "print(\"Age min/max after enforcement:\", df_enforced[\"age\"].min(), df_enforced[\"age\"].max())\n",
        "\n",
        "# Schema evolution example: add a column and widen allowed set\n",
        "schema_v2 = infer_schema(df_enforced.assign(country=\"US\", loyalty=\"silver\"), primary_key=[\"id\"], description=\"Users v2\", version=\"2.0\")\n",
        "diff = evolve_schema(schema_loaded, schema_v2)\n",
        "print(\"\\nSchema evolution diff:\")\n",
        "print(diff)\n",
        "\n",
        "# DQ checks\n",
        "print(\"\\nUniqueness check on ['id']:\")\n",
        "print(check_uniqueness(df_new, [\"id\"]))\n",
        "\n",
        "# Referential integrity: build parent (countries) and child (users)\n",
        "parent = pd.DataFrame({\"country\": [\"US\", \"UK\", \"CA\"]})\n",
        "ri = check_referential_integrity(df_new, parent, fk_cols=[\"country\"], pk_cols=[\"country\"])\n",
        "print(\"\\nReferential integrity (country):\", ri)\n",
        "\n",
        "# Distribution drift (e.g., age/country changed)\n",
        "drift = detect_distribution_drift(df_new, df_ref, columns=[\"age\", \"country\"])\n",
        "print(\"\\nDistribution drift:\")\n",
        "print(drift)\n"
      ],
      "metadata": {
        "id": "9Th7d7BN3mo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}