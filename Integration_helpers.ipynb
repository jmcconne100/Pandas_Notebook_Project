{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWqi2cVv4MklYgF9LF3Axb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmcconne100/Pandas_Notebook_Project/blob/main/Integration_helpers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_AV-vn1fDlf"
      },
      "outputs": [],
      "source": [
        "# integration_helpers.py\n",
        "from __future__ import annotations\n",
        "import io\n",
        "import json\n",
        "import gzip\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---- S3 ----\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import BotoCoreError, ClientError\n",
        "\n",
        "# ---- SQL ----\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.engine import Engine\n",
        "\n",
        "\n",
        "# ---------- S3: Write DataFrame as JSON / JSON Lines ----------\n",
        "def df_to_json_s3(\n",
        "    df: pd.DataFrame,\n",
        "    bucket: str,\n",
        "    key: str,\n",
        "    *,\n",
        "    json_orient: str = \"records\",      # 'records' or 'table' (for whole JSON doc)\n",
        "    json_lines: bool = False,          # True => JSON Lines (one object per line)\n",
        "    gzip_compress: bool = False,\n",
        "    content_type: Optional[str] = None,\n",
        "    s3_client: Optional[Any] = None,\n",
        "    extra_put_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    region_name: Optional[str] = None,\n",
        "    max_attempts: int = 5,\n",
        "    timeout_s: int = 30,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Serialize a DataFrame to JSON or JSON Lines and upload to S3.\n",
        "\n",
        "    Example:\n",
        "        df_to_json_s3(df, \"my-bucket\", \"exports/data.json.gz\",\n",
        "                      json_orient=\"records\", json_lines=True, gzip_compress=True)\n",
        "    \"\"\"\n",
        "    # Serialize\n",
        "    if json_lines:\n",
        "        # Each row -> one JSON object line\n",
        "        buf = io.StringIO()\n",
        "        df.to_json(buf, orient=\"records\", lines=True)  # pandas handles newlines\n",
        "        payload = buf.getvalue().encode(\"utf-8\")\n",
        "        default_ct = \"application/x-ndjson\"\n",
        "    else:\n",
        "        obj = json.loads(df.to_json(orient=json_orient))\n",
        "        payload = json.dumps(obj, ensure_ascii=False).encode(\"utf-8\")\n",
        "        default_ct = \"application/json\"\n",
        "\n",
        "    # Optional gzip\n",
        "    if gzip_compress:\n",
        "        gzbuf = io.BytesIO()\n",
        "        with gzip.GzipFile(fileobj=gzbuf, mode=\"wb\") as gz:\n",
        "            gz.write(payload)\n",
        "        payload = gzbuf.getvalue()\n",
        "        default_ct = default_ct  # content-type stays JSON; encoding signals gzip\n",
        "\n",
        "    # S3 client with retries/timeouts\n",
        "    s3 = s3_client or boto3.client(\n",
        "        \"s3\",\n",
        "        region_name=region_name,\n",
        "        config=Config(retries={\"max_attempts\": max_attempts, \"mode\": \"standard\"},\n",
        "                      connect_timeout=timeout_s, read_timeout=timeout_s),\n",
        "    )\n",
        "\n",
        "    put_kwargs = extra_put_kwargs.copy() if extra_put_kwargs else {}\n",
        "    put_kwargs.setdefault(\"Bucket\", bucket)\n",
        "    put_kwargs.setdefault(\"Key\", key)\n",
        "    put_kwargs.setdefault(\"Body\", payload)\n",
        "\n",
        "    # Headers\n",
        "    ct = content_type or default_ct\n",
        "    put_kwargs.setdefault(\"ContentType\", ct)\n",
        "    if gzip_compress:\n",
        "        put_kwargs.setdefault(\"ContentEncoding\", \"gzip\")\n",
        "\n",
        "    try:\n",
        "        resp = s3.put_object(**put_kwargs)\n",
        "        return {\"bucket\": bucket, \"key\": key, \"etag\": resp.get(\"ETag\")}\n",
        "    except (BotoCoreError, ClientError) as e:\n",
        "        raise RuntimeError(f\"S3 upload failed for s3://{bucket}/{key}: {e}\") from e\n",
        "\n",
        "\n",
        "# ---------- S3: Read CSV into DataFrame ----------\n",
        "def read_s3_csv(\n",
        "    bucket: str,\n",
        "    key: str,\n",
        "    *,\n",
        "    s3_client: Optional[Any] = None,\n",
        "    pandas_read_csv_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    region_name: Optional[str] = None,\n",
        "    max_attempts: int = 5,\n",
        "    timeout_s: int = 30,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read a CSV object from S3 (auto-handling bytes â†’ pandas).\n",
        "\n",
        "    Example:\n",
        "        df = read_s3_csv(\"my-bucket\", \"landing/sales_2025-10-17.csv\",\n",
        "                         pandas_read_csv_kwargs={\"dtype\": {\"id\": \"Int64\"}})\n",
        "    \"\"\"\n",
        "    s3 = s3_client or boto3.client(\n",
        "        \"s3\",\n",
        "        region_name=region_name,\n",
        "        config=Config(retries={\"max_attempts\": max_attempts, \"mode\": \"standard\"},\n",
        "                      connect_timeout=timeout_s, read_timeout=timeout_s),\n",
        "    )\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        body = obj[\"Body\"].read()\n",
        "    except (BotoCoreError, ClientError) as e:\n",
        "        raise RuntimeError(f\"S3 download failed for s3://{bucket}/{key}: {e}\") from e\n",
        "\n",
        "    buf = io.BytesIO(body)\n",
        "    kwargs = pandas_read_csv_kwargs or {}\n",
        "    try:\n",
        "        return pd.read_csv(buf, **kwargs)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to parse CSV from s3://{bucket}/{key}: {e}\") from e\n",
        "\n",
        "\n",
        "# ---------- SQL: Write DataFrame to Database ----------\n",
        "def write_sql(\n",
        "    df: pd.DataFrame,\n",
        "    conn_str: str | Engine,\n",
        "    table: str,\n",
        "    *,\n",
        "    schema: Optional[str] = None,\n",
        "    if_exists: str = \"replace\",   # 'fail' | 'replace' | 'append'\n",
        "    index: bool = False,\n",
        "    chunksize: Optional[int] = 10_000,\n",
        "    method: Optional[str] = None, # e.g., 'multi' for faster inserts on some DBs\n",
        "    dtype: Optional[Dict[str, Any]] = None,\n",
        "    create_table_sql: Optional[str] = None,  # optional DDL to run before write\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Write a DataFrame to a SQL table using SQLAlchemy.\n",
        "\n",
        "    conn_str examples:\n",
        "      - 'postgresql+psycopg2://user:pass@host:5432/dbname'\n",
        "      - 'sqlite:///local.db'\n",
        "      - Engine instance from create_engine(...)\n",
        "\n",
        "    Example:\n",
        "        write_sql(df, \"sqlite:///demo.db\", \"sales\", if_exists=\"append\", chunksize=5000)\n",
        "    \"\"\"\n",
        "    engine: Engine = conn_str if isinstance(conn_str, Engine) else create_engine(conn_str)\n",
        "    try:\n",
        "        with engine.begin() as conn:\n",
        "            if create_table_sql:\n",
        "                conn.execute(text(create_table_sql))\n",
        "            df.to_sql(\n",
        "                name=table,\n",
        "                con=conn,\n",
        "                if_exists=if_exists,\n",
        "                index=index,\n",
        "                schema=schema,\n",
        "                chunksize=chunksize,\n",
        "                method=method,\n",
        "                dtype=dtype,\n",
        "            )\n",
        "    finally:\n",
        "        # Dispose only if we created it here (simple heuristic)\n",
        "        if not isinstance(conn_str, Engine):\n",
        "            engine.dispose()\n"
      ]
    }
  ]
}